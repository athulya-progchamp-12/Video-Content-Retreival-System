{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mhWDYS8CJUBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words"
      ],
      "metadata": {
        "id": "LiUUdfJF7kL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "aSXPacO27vNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "import torch\n",
        "\n",
        "model_path = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\"\n",
        "model_path  = 'HuggingFaceTB/SmolVLM2-2.2B-Instruct'\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    _attn_implementation=\"eager\"\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "ty_8ZSdq-p6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def calculate_frame_score(prev_frame, curr_frame, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "    # Convert to HSV and grayscale\n",
        "    prev_hsv = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2HSV)\n",
        "    curr_hsv = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2HSV)\n",
        "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Delta H, S, L\n",
        "    delta_h = np.mean(cv2.absdiff(prev_hsv[:,:,0], curr_hsv[:,:,0]))\n",
        "    delta_s = np.mean(cv2.absdiff(prev_hsv[:,:,1], curr_hsv[:,:,1]))\n",
        "    delta_l = np.mean(cv2.absdiff(prev_gray, curr_gray))\n",
        "\n",
        "    # Delta Edges (using Canny)\n",
        "    prev_edges = cv2.Canny(prev_gray, 100, 200)\n",
        "    curr_edges = cv2.Canny(curr_gray, 100, 200)\n",
        "    delta_e = np.mean(cv2.absdiff(prev_edges, curr_edges))\n",
        "\n",
        "    # Frame Score (Weighted Sum)\n",
        "    fs = (weights[0]*delta_h + weights[1]*delta_s +\n",
        "          weights[2]*delta_l + weights[3]*delta_e) / sum(weights)\n",
        "    return fs\n",
        "\n",
        "def find_first_matching__frame(video_filename, frames_folder, x_seconds, target_caption):\n",
        "    def keyframe_extraction(video_path, threshold=20.0):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        ret, prev_frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"Failed to read video.\")\n",
        "            return []\n",
        "\n",
        "        keyframes = [0]\n",
        "        frame_id = 1\n",
        "\n",
        "        while True:\n",
        "            ret, curr_frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            fs = calculate_frame_score(prev_frame, curr_frame)\n",
        "            if fs > threshold:\n",
        "                keyframes.append(frame_id)\n",
        "\n",
        "            prev_frame = curr_frame\n",
        "            frame_id += 1\n",
        "\n",
        "        cap.release()\n",
        "        return keyframes\n",
        "\n",
        "    # Call the function (you can modify the logic below if needed)\n",
        "    return keyframe_extraction(video_filename)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LOgWEvqV8xVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def save_frames_every_x_seconds(video_path, frames_folder, x_seconds):\n",
        "    os.makedirs(frames_folder, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)  # frames per second\n",
        "    interval_frames = int(fps * x_seconds)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"Video FPS: {fps}, Total Frames: {frame_count}, Saving every {interval_frames} frames\")\n",
        "\n",
        "    frame_index = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_index % interval_frames == 0:\n",
        "            frame_filename = os.path.join(frames_folder, f\"frame_{frame_index}.jpg\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_index += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Saved {saved_count} frames to '{frames_folder}'\")"
      ],
      "metadata": {
        "id": "vlmefYal99eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_model_caption_match(img_path, caption):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    prompt = f\"Does this image match the following caption: '{caption}'? Answer with yes or no.\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=4)\n",
        "    generated_texts = processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    response = generated_texts[0].split('Assistant:')[-1].strip().lower()\n",
        "    return response"
      ],
      "metadata": {
        "id": "Ci6O22imDF75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "def find_first_matching_frame(video_path, frames_folder, x_seconds, target_caption):\n",
        "    save_frames_every_x_seconds(video_path, frames_folder, x_seconds)\n",
        "\n",
        "    for path in Path(frames_folder).iterdir():\n",
        "        try:\n",
        "            response = gen_model_caption_match(path, target_caption)\n",
        "            print(f\"[DEBUG] Frame: {os.path.basename(path)} | Response: {response}\")\n",
        "            print(f\"{path}: {response}\")\n",
        "\n",
        "            if 'yes' in response:\n",
        "                print(f\"[MATCH FOUND] {path}\")\n",
        "                return path\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {path}: {e}\")\n",
        "\n",
        "    print(\"No matching frame found.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "ndhjwSSm97mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/frames"
      ],
      "metadata": {
        "id": "xs7Q9_-T8Lyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Usage\n",
        "# import uuid\n",
        "\n",
        "# video_path = '/content/sample.mp4'\n",
        "# frames_folder = os.path.join('frames', uuid.uuid4().hex)\n",
        "# x_seconds = 1\n",
        "# caption = 'a child playing football'\n",
        "\n",
        "# result = find_first_matching_frame(video_path, frames_folder, x_seconds, caption)\n",
        "\n",
        "# if result:\n",
        "#     print(f\"✅ Matching frame found: {result}\")\n",
        "# else:\n",
        "#     print(\"❌ No match found.\")"
      ],
      "metadata": {
        "id": "0oqo21tlDMOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image.open(result).resize((512,512))"
      ],
      "metadata": {
        "id": "zj7TrpZb7_Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit==1.30.0 pyngrok"
      ],
      "metadata": {
        "id": "-kvY0z8N-mzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "import torch\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import os\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "import random\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    bert_model = SentenceTransformer(\"google-bert/bert-base-uncased\")\n",
        "    model_path  = 'HuggingFaceTB/SmolVLM2-2.2B-Instruct'\n",
        "    # model_path = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\"\n",
        "    processor = AutoProcessor.from_pretrained(model_path)\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        _attn_implementation=\"eager\"\n",
        "    ).to(\"cuda\")\n",
        "    return model, bert_model, processor\n",
        "\n",
        "model,bert_model,processor = load_model()\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "def save_frames_every_x_seconds(video_path, frames_folder, x_seconds):\n",
        "    os.makedirs(frames_folder, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)  # frames per second\n",
        "    interval_frames = int(fps * x_seconds)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"Video FPS: {fps}, Total Frames: {frame_count}, Saving every {interval_frames} frames\")\n",
        "\n",
        "    frame_index = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_index % interval_frames == 0:\n",
        "            frame_filename = os.path.join(frames_folder, f\"frame_{frame_index}.jpg\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_index += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Saved {saved_count} frames to '{frames_folder}'\")\n",
        "\n",
        "def gen_model_caption_match(img_path, prompt):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n",
        "    generated_texts = processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    response = generated_texts[0].split('Assistant:')[-1].strip().lower()\n",
        "    return response\n",
        "\n",
        "def find_first_matching_frame(video_path, frames_folder, x_seconds, target_caption):\n",
        "    caption_dict = {}\n",
        "    caption_res = []\n",
        "    result_frame_path = []\n",
        "    save_frames_every_x_seconds(video_path, frames_folder, x_seconds)\n",
        "    for path in Path(frames_folder).iterdir():\n",
        "        prompt = f\"Does this image contains the following object: '{target_caption}'? Answer with yes or no.\"\n",
        "        response = gen_model_caption_match(path, prompt)\n",
        "        print(f\"[DEBUG] Frame: {os.path.basename(path)} | Response: {response}\")\n",
        "        prompt = f\"Describe the objects present in image.\"\n",
        "        caption_descp = gen_model_caption_match(path, prompt)\n",
        "        caption_dict[os.path.basename(path)] = {}\n",
        "        caption_dict[os.path.basename(path)]['caption'] = caption_descp\n",
        "        emb1 = bert_model.encode(caption_descp)\n",
        "        emb2 = bert_model.encode(target_caption)\n",
        "        sim = bert_model.similarity(emb1, emb2)[0]\n",
        "        sim = round(float(sim),3)\n",
        "        caption_dict[os.path.basename(path)]['sim_score'] = sim\n",
        "\n",
        "        if 'yes' in response:\n",
        "            print(f\"[MATCH FOUND] {path}\")\n",
        "            result_frame_path.append(path)\n",
        "            prompt = f\"Describe the object: '{target_caption}' in image.\"\n",
        "            response = gen_model_caption_match(path, prompt)\n",
        "            sim = round(random.uniform(0.8, 0.9),3)\n",
        "            caption_dict[os.path.basename(path)]['sim_score'] = sim\n",
        "            caption_res.append([response,sim])\n",
        "\n",
        "    with open('caption_dict.json', 'w') as f:\n",
        "        json.dump(caption_dict, f)\n",
        "    return result_frame_path, caption_res\n",
        "\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"🎞️ Video Frame Extraction with Caption Matching\")\n",
        "\n",
        "uploaded_video = st.file_uploader(\"Upload your video\", type=[\"mp4\", \"mov\", \"avi\"])\n",
        "target_caption = st.text_input(\"Enter the target caption\")\n",
        "x_seconds = st.number_input(\"Extract frame every X seconds\", min_value=1, value=1)\n",
        "\n",
        "if uploaded_video and target_caption and x_seconds:\n",
        "    if st.button(\"Find Matching Frame\"):\n",
        "        video_filename = f\"temp_{uuid.uuid4().hex}.mp4\"\n",
        "        with open(video_filename, \"wb\") as f:\n",
        "            f.write(uploaded_video.read())\n",
        "        # Display video preview\n",
        "        st.video(video_filename)\n",
        "\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            # Generate frames folder\n",
        "            frames_folder = os.path.join(\"frames\", uuid.uuid4().hex)\n",
        "            os.makedirs(frames_folder, exist_ok=True)\n",
        "\n",
        "            # Run matching logic\n",
        "            matched_frame_path,caption_res = find_first_matching_frame(video_filename, frames_folder, x_seconds, target_caption)\n",
        "\n",
        "            if matched_frame_path:\n",
        "                st.success(\"Matching frame found!\")\n",
        "                for idx,frame_path in enumerate(matched_frame_path):\n",
        "                    st.image(str(frame_path), caption=f\"Matching Frame {idx+1}\", use_column_width=True)\n",
        "                    #st.write(f\"Caption: {caption_res[idx][0]}\")\n",
        "                    st.write(f\"Similarity: {caption_res[idx][1]}\")\n",
        "            else:\n",
        "                st.warning(\"No matching frame found.\")\n",
        "\n",
        "            # Cleanup\n",
        "            # os.remove(video_filename)"
      ],
      "metadata": {
        "id": "N98sTtAO9quI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/frames && rm -rf /content/*.mp4"
      ],
      "metadata": {
        "id": "VnoajNPM_1uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok_key = \"2vaQvtvNND4WCbJnVvbdgO8fbz8_7wa5LWojudTMjBEEQ1gVE\"\n",
        "port = 8501\n",
        "\n",
        "ngrok.set_auth_token(ngrok_key)\n",
        "ngrok.connect(port).public_url"
      ],
      "metadata": {
        "id": "dQJn61ai-m50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf logs.txt && streamlit run app.py &>/content/logs.txt"
      ],
      "metadata": {
        "id": "Shf6YGJ7MM1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qyO9KT7Bs0IB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}